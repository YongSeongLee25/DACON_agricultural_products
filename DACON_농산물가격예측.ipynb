{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./다운로드/235801_2021 농산물 가격예측 AI 경진대회/sample_submission.csv')\n",
    "public_date_list = submission[submission['예측대상일자'].str.contains('2021')]['예측대상일자'].str.split('+').str[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기, 값이 0이면 NAN 변경후 다음날값으로 채우기 없으면 전날값\n",
    "data = pd.read_csv('real_data.csv')\n",
    "data = data.replace(0, np.NaN)\n",
    "data = data.interpolate()\n",
    "data = data.fillna(method='bfill')\n",
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pum = [\n",
    "    '배추', '무', '양파', '건고추','마늘',\n",
    "    '대파', '얼갈이배추', '양배추', '깻잎',\n",
    "    '시금치', '미나리', '당근',\n",
    "    '파프리카', '새송이', '팽이버섯', '토마토',\n",
    "    '청상추', '백다다기', '애호박', '캠벨얼리', '샤인마스캇'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요없는 피쳐 제거\n",
    "or pum in unique_pum:\n",
    "    data.drop([f'{pum}_QTYs',f'{pum}_QTYm', f'{pum}_DANQs', f'{pum}_DANQm'], axis=1, inplace=True)\n",
    "    data.drop(f'{pum}_거래량(kg)', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_col = list(data.columns[1:169:8])\n",
    "deal_data = data.iloc[:,1:169:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for unique in unique_pum:\n",
    "    data.drop([f'{unique}_SAN_NM', f'{unique}_WHSAL_NM', f'{unique}_CMP_NM', f'{unique}_KIND_NM', f'{unique}_LV_NM'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일, 일, 년도별월, 년도, 주 추출\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['Weekday'] = data['date'].dt.weekday\n",
    "data['YearMonth'] = data['date'].dt.to_period('M')\n",
    "data['Year'] = data['date'].dt.to_period('Y')\n",
    "data['Week'] = data['date'].dt.week\n",
    "\n",
    "# 단위 별로 데이터 그룹생성\n",
    "gr_data = data.groupby(data['Week'])\n",
    "gr_data_day = data.groupby(data['Weekday'])\n",
    "gr_data_month = data.groupby(data['YearMonth'])\n",
    "gr_data_year = data.groupby(data['Year'])\n",
    "gr_data_mon_day = data.groupby([data['YearMonth'], data['Weekday']])\n",
    "\n",
    "# 단위 별 평균, 합, 표준편차\n",
    "group_datas = pd.concat([\n",
    "    gr_data[list_col].mean().rename(columns = lambda x : 'week_means_' + x),\n",
    "    gr_data[list_col].sum().rename(columns = lambda x : 'week_sum_' + x),\n",
    "    gr_data[list_col].std().rename(columns = lambda x : 'week_max_' + x)\n",
    "], axis = 1).reset_index()\n",
    "\n",
    "group_datas_day = pd.concat([\n",
    "    gr_data_day[list_col].mean().rename(columns = lambda x : 'day_means_' + x),\n",
    "    gr_data_day[list_col].sum().rename(columns = lambda x : 'day_sum_' + x),\n",
    "    gr_data_day[list_col].std().rename(columns = lambda x : 'day_max_' + x)\n",
    "], axis = 1).reset_index()\n",
    "\n",
    "group_datas_month = pd.concat([\n",
    "    gr_data_month[list_col].mean().rename(columns = lambda x : 'month_means_' + x),\n",
    "    gr_data_month[list_col].sum().rename(columns = lambda x : 'month_sum_' + x),\n",
    "    gr_data_month[list_col].std().rename(columns = lambda x : 'month_max_' + x)\n",
    "], axis = 1).reset_index()\n",
    "\n",
    "group_datas_year = pd.concat([\n",
    "    gr_data_year[list_col].mean().rename(columns = lambda x : 'year_means_' + x),\n",
    "    gr_data_year[list_col].sum().rename(columns = lambda x : 'year_sum_' + x),\n",
    "    gr_data_year[list_col].std().rename(columns = lambda x : 'year_max_' + x)\n",
    "], axis = 1).reset_index()\n",
    "\n",
    "group_datas_mon_day = pd.concat([\n",
    "    gr_data_mon_day[list_col].mean().rename(columns = lambda x : 'mon_day_means_' + x),\n",
    "    gr_data_mon_day[list_col].sum().rename(columns = lambda x : 'mon_day_sum_' + x),\n",
    "    gr_data_mon_day[list_col].std().rename(columns = lambda x : 'mon_day_max_' + x)\n",
    "], axis = 1).reset_index()\n",
    "\n",
    "# 데이터 모두 합치기\n",
    "data = pd.merge(data, group_datas, on = 'Week', how = 'left')\n",
    "data = pd.merge(data, group_datas_day, on = 'Weekday', how = 'left')\n",
    "data = pd.merge(data, group_datas_month, on = 'YearMonth', how = 'left')\n",
    "data = pd.merge(data, group_datas_year, on = 'Year', how = 'left')\n",
    "data = pd.merge(data, group_datas_mon_day, on = ['YearMonth','Weekday'], how = 'left')\n",
    "data.drop(['Week', 'YearMonth', 'Weekday','Year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_data = []\n",
    "# 평균,합,표준편차 값과 농산물가격과 나눠 해당 값들과 영향력있는 퍼센트 피쳐 생성\n",
    "for col in list_col:\n",
    "    data[col + 'week_mean'] = data[col] / data['week_means_' + col]\n",
    "    data[col + 'week_sum'] = data[col] / data['week_sum_' + col]\n",
    "    data[col + 'week_max'] = data[col] / data['week_max_' + col]\n",
    "    \n",
    "    data[col + 'day_mean'] = data[col] / data['day_means_' + col]\n",
    "    data[col + 'day_sum'] = data[col] / data['day_sum_' + col]\n",
    "    data[col + 'day_max'] = data[col] / data['day_max_' + col]\n",
    "    \n",
    "    data[col + 'month_mean'] = data[col] / data['month_means_' + col]\n",
    "    data[col + 'month_sum'] = data[col] / data['month_sum_' + col]\n",
    "    data[col + 'month_max'] = data[col] / data['month_max_' + col]\n",
    "    \n",
    "    data[col + 'year_mean'] = data[col] / data['year_means_' + col]\n",
    "    data[col + 'year_sum'] = data[col] / data['year_sum_' + col]\n",
    "    data[col + 'year_max'] = data[col] / data['year_max_' + col]\n",
    "    \n",
    "    data[col + 'mon_day_mean'] = data[col] / data['mon_day_means_' + col]\n",
    "    data[col + 'mon_day_sum'] = data[col] / data['mon_day_sum_' + col]\n",
    "    data[col + 'mon_day_max'] = data[col] / data['mon_day_max_' + col]\n",
    "    \n",
    "    # 평균,합,표준편차 값 제거\n",
    "    drop_data = ['week_means_' + col, 'week_sum_' + col, 'week_max_' + col, 'day_means_' + col, 'day_sum_' + col, 'day_max_' + col, 'month_means_'+col, 'month_sum_'+col,\n",
    "              'month_max_' + col, 'year_means_' + col, 'year_sum_' + col, 'year_max_' + col, 'mon_day_means_' + col, 'mon_day_sum_' + col, 'mon_day_max_' + col]\n",
    "    data.drop(drop_data,\n",
    "              axis=1, inplace=True)\n",
    "\n",
    "    # 음양의 무한대값이 나왔을경우 다으날 또는 전날값으로 대체\n",
    "data = data.replace([np.inf, -np.inf], np.NaN)\n",
    "data = data.fillna(method='bfill')\n",
    "data = data.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_result = []\n",
    "col1_count_first = 1\n",
    "col1_count_two = 4\n",
    "\n",
    "col2_count_first = 64\n",
    "col2_count_two = 79\n",
    "\n",
    "for i in range(len(unique_pum)):\n",
    "    col1 = data.columns[col1_count_first:col1_count_two].to_list()\n",
    "    col2 = data.columns[col2_count_first:col2_count_two].to_list()\n",
    "\n",
    "    col = col1 + col2\n",
    "    col_result.append(col)\n",
    "    \n",
    "    col1_count_first = col1_count_two\n",
    "    col1_count_two += 3\n",
    "    \n",
    "    col2_count_first = col2_count_two\n",
    "    col2_count_two += 15\n",
    "    \n",
    "answer = sum(col_result, [])\n",
    "answer.insert(0, 'date')\n",
    "data = data[answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmax\n",
    "norm = data.iloc[:, 1:].max(0)\n",
    "data.iloc[:, 1:] = data.iloc[:, 1:] / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터\n",
    "df_test = data.iloc[1781:1811].reset_index(drop=True)\n",
    "test_df = data.iloc[1811:].reset_index(drop=True)\n",
    "\n",
    "# 훈련 데이터\n",
    "data = data.iloc[:1781]\n",
    "deal_data = data.iloc[:,1:694:18]\n",
    "\n",
    "# 검증 데이터\n",
    "df_test_deal_data = df_test.iloc[:,1:694:18]\n",
    "test_deal_data = test_df.iloc[:,1:694:18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "target_n = 1\n",
    "learning_rate = 5e-3\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 15\n",
    "teacher_forcing = False\n",
    "n_layers = 2\n",
    "dropout = 0.2 \n",
    "window_size = 28\n",
    "future_size = 28\n",
    "hidden_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_date_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.encoder_input)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'encoder_input' : torch.tensor(self.encoder_input[i], dtype=torch.float32),\n",
    "            'decoder_input' : torch.tensor(self.decoder_input[i], dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.rnn = nn.GRU(input_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inp_seq):\n",
    "        inp_seq = inp_seq.permute(1,0,2)\n",
    "        outputs, hidden = self.rnn(inp_seq)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, dec_output_dim, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(dec_output_dim, units)\n",
    "        self.W2 = nn.Linear(dec_output_dim, units)\n",
    "        self.V = nn.Linear(dec_output_dim, 1)\n",
    "\n",
    "    def forward(self, hidden, enc_output):\n",
    "        query_with_time_axis = hidden.unsqueeze(1)\n",
    "        \n",
    "        score = self.V(torch.tanh(self.W1(query_with_time_axis) + self.W2(enc_output)))\n",
    "        \n",
    "        attention_weights = torch.softmax(score, axis=1)\n",
    "        \n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = torch.sum(context_vector, dim=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_feature_size, encoder_hidden_dim, output_dim, decoder_hidden_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.decoder_hidden_dim = decoder_hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.layer = nn.Linear(dec_feature_size, encoder_hidden_dim)\n",
    "        self.rnn = nn.GRU(encoder_hidden_dim*2, decoder_hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, enc_output, dec_input, hidden):\n",
    "        dec_input = self.layer(dec_input)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        dec_input = torch.cat([torch.sum(context_vector, dim=0), dec_input], dim=1)\n",
    "        dec_input = dec_input.unsqueeze(0)\n",
    "        \n",
    "        output, hidden = self.rnn(dec_input, hidden)\n",
    "        prediction = self.fc_out(output.sum(0))\n",
    "\n",
    "        \n",
    "        return prediction, hidden\n",
    "\n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, encoder_input, decoder_input, teacher_forcing=False):\n",
    "        batch_size = decoder_input.size(0)\n",
    "        trg_len = decoder_input.size(1)\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len-1, self.decoder.output_dim).to(device)\n",
    "        enc_output, hidden = self.encoder(encoder_input)\n",
    "        \n",
    "        dec_input = decoder_input[:, 0] # [:, 0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(enc_output, dec_input, hidden)\n",
    "            outputs[:, t-1] = output\n",
    "            if teacher_forcing == True:\n",
    "                dec_input = decoder_input[:, t]\n",
    "            else:\n",
    "                dec_input = output\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "\n",
    "def my_custom_metric(pred, true):\n",
    "    pred = pred[:, [6, 13, 27]]\n",
    "    true = true[:, [6, 13, 27]]\n",
    "    target = torch.where(true!=0)\n",
    "    true = true[target]\n",
    "    pred = pred[target]\n",
    "    score = torch.mean(torch.abs((true-pred))/(true))\n",
    "    \n",
    "    return score    \n",
    "\n",
    "def train_step(batch_item, epoch, batch, training, teacher_forcing):\n",
    "    encoder_input = batch_item['encoder_input'].to(device)\n",
    "    decoder_input = batch_item['decoder_input'].to(device)\n",
    "    if training is True:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(encoder_input, decoder_input, teacher_forcing)\n",
    "            loss = criterion(output, decoder_input[:, 1:])\n",
    "            score = custom_metric(output, decoder_input[:, 1:])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        return loss, score\n",
    "    else:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(encoder_input, decoder_input, False)\n",
    "            loss = criterion(output, decoder_input[:, 1:])\n",
    "            score = custom_metric(output, decoder_input[:, 1:])\n",
    "        return loss, score\n",
    "\n",
    "def predict(encoder_input):\n",
    "    model.train()\n",
    "    encoder_input = encoder_input.to(device)\n",
    "    decoder_input = torch.zeros([1, future_size+2, target_n], dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(encoder_input, decoder_input, False)\n",
    "    return output.cpu()\n",
    "\n",
    "# 농산물 별로 각각의 모델 추론\n",
    "pum_len = len(unique_pum)\n",
    "count = 1\n",
    "count_f = 19 # 29\n",
    "for pum in range(pum_len):\n",
    "    # 모델 불러오기\n",
    "    save_path = f'./models/{unique_pum[pum]}best_model.pt'\n",
    "    model = torch.load(save_path)\n",
    "    model = model.to(device)\n",
    "    public_date_list = submission[submission['예측대상일자'].str.contains('2021')]['예측대상일자'].str.split('+').str[0].unique()\n",
    "    outputs = []\n",
    "    troch_norm = torch.tensor(norm.to_numpy()[count-1])\n",
    "    for index in range(len(test_df)-1):\n",
    "        data_df = pd.concat([df_test, test_df.iloc[:index+1]]).iloc[-window_size:].reset_index(drop=True)\n",
    "        deal_data_df = pd.concat([df_test_deal_data, test_deal_data.iloc[:index+1]]).iloc[-window_size:].reset_index(drop=True)\n",
    "\n",
    "        data_df = data_df.iloc[:,count:count_f]\n",
    "        data_df = pd.merge(data_df, deal_data_df, left_index=True, right_index=True, how='left')\n",
    "        data_df.drop(f'{unique_pum[pum]}_가격(원/kg)_x', axis=1, inplace=True)\n",
    "        \n",
    "        encoder_input = torch.tensor(data_df.to_numpy(), dtype=torch.float32)\n",
    "        encoder_input = encoder_input.unsqueeze(0)\n",
    "        \n",
    "        output = predict(encoder_input) * troch_norm\n",
    "        idx = submission[submission['예측대상일자'].str.contains(public_date_list[index])].index\n",
    "        # 당일데이터를 사용할수 없기 때문에 전날 데이터로 8, 15, 29일 후 추론값 저장\n",
    "        submission.loc[idx, f'{unique_pum[pum]}_가격(원/kg)'] = output[0, [7,14,28]].numpy()\n",
    "    count = count_f\n",
    "    count_f += 18\n",
    "submission.to_csv('dacon_inference/dacons.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.tail(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_result = pd.read_csv('./다운로드/235801_2021 농산물 가격예측 AI 경진대회/sample_submission.csv')\n",
    "\n",
    "count_save = 0\n",
    "count = 1\n",
    "count_f = 19 # 29\n",
    "\n",
    "pum_len = len(unique_pum)\n",
    "\n",
    "for pum in range(pum_len):\n",
    "    save_path = f'./models/{unique_pum[pum]}best_model.pt'\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for i in range(data.shape[0]-window_size-future_size):\n",
    "        # 원래 피쳐와 기존 농산물가격 결합\n",
    "        copy_data = pd.merge(data.iloc[:, count:count_f], deal_data, left_index=True, right_index=True, how='left')\n",
    "        copy_data.drop(f'{unique_pum[pum]}_가격(원/kg)_x', axis=1, inplace=True)\n",
    "#         x = data.iloc[i:i+window_size, count:count_f].to_numpy() # count:count_f\n",
    "        x = copy_data.iloc[i:i+window_size, :].to_numpy()\n",
    "        y = data.iloc[i+window_size:i+window_size+future_size, count: count+1].to_numpy() # count+1 , count +2\n",
    "        y_0 = np.zeros([1,1])\n",
    "        x_data.append(x)\n",
    "        y_data.append(np.concatenate([y_0, y], axis=0))\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "\n",
    "    train_test_split = 1\n",
    "    x_train = x_data[:-train_test_split-future_size]\n",
    "    y_train = y_data[:-train_test_split-future_size]\n",
    "    x_val = x_data[-train_test_split:]\n",
    "    y_val = y_data[-train_test_split:]\n",
    "\n",
    "    train_dataset = CustomDataset(x_train, y_train)\n",
    "    val_dataset = CustomDataset(x_val, y_val)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=16, shuffle=False)\n",
    "    sample_batch = next(iter(train_dataloader))\n",
    "\n",
    "    encoder = Encoder(input_dim=x_data.shape[-1], hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout)\n",
    "    attention = BahdanauAttention(dec_output_dim=hidden_dim, units=hidden_dim)\n",
    "    decoder = Decoder(\n",
    "        dec_feature_size=target_n, encoder_hidden_dim=hidden_dim, output_dim=target_n,\n",
    "        decoder_hidden_dim=hidden_dim, n_layers=n_layers, dropout=dropout,\n",
    "        attention = attention\n",
    "    )\n",
    "\n",
    "    model = Seq2Seq(encoder, decoder, attention)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.L1Loss() # mae\n",
    "    custom_metric = my_custom_metric\n",
    "\n",
    "\n",
    "    loss_plot, val_loss_plot = [], []\n",
    "    score_plot, val_score_plot = [], []\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss, total_val_loss = 0, 0\n",
    "        total_score, total_val_score = 0, 0\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
    "        training = True\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_score = train_step(batch_item, epoch, batch, training, teacher_forcing)\n",
    "            total_loss += batch_loss\n",
    "            total_score += batch_score\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Total Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
    "                'Score': '{:06f}'.format(batch_score.item()),\n",
    "                'Total Score' : '{:06f}'.format(total_score/(batch+1)),\n",
    "            })\n",
    "    #     total_loss = total_loss.cpu().numpy()\n",
    "    #     total_score = total_score.cpu().numpy()\n",
    "        loss_plot.append(total_loss/(batch+1))\n",
    "        score_plot.append(total_score/(batch+1))\n",
    "\n",
    "        tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
    "        training = False\n",
    "        for batch, batch_item in tqdm_dataset:\n",
    "            batch_loss, batch_val_score = train_step(batch_item, epoch, batch, training, teacher_forcing)\n",
    "            total_val_loss += batch_loss\n",
    "            total_val_score += batch_val_score\n",
    "\n",
    "            tqdm_dataset.set_postfix({\n",
    "                'Epoch': epoch + 1,\n",
    "                'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
    "                'Total Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
    "                'Val Score': '{:06f}'.format(batch_val_score.item()),\n",
    "                'Total Val Score' : '{:06f}'.format(total_val_score/(batch+1)),\n",
    "            })\n",
    "        \n",
    "   \n",
    "        total_val_loss = total_val_loss.cpu().numpy()\n",
    "        total_val_score = total_val_score.cpu().numpy()\n",
    "        val_loss_plot.append(total_val_loss/(batch+1))\n",
    "        val_score_plot.append(total_val_score/(batch+1))\n",
    "        \n",
    "        if np.min(val_loss_plot) == val_loss_plot[-1]:\n",
    "            torch.save(model, save_path)\n",
    "\n",
    "    print(f'{unique_pum[pum]}')\n",
    "    count = count_f\n",
    "    count_f += 18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
